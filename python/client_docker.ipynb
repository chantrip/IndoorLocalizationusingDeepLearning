{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import ssl\n",
    "import platform\n",
    "import time\n",
    "import sys\n",
    "import time\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#'http://novasoft-th.com/indoor'\n",
    "try:  \n",
    "    os.environ[\"url\"]\n",
    "    url = os.environ[\"url\"]\n",
    "except KeyError: \n",
    "    print(\"Url is not set.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "def bouncingSleep(retryCount):\n",
    "    waittime = 8 if (retryCount[0])**2 > 8 else (retryCount[0])**2\n",
    "    if((retryCount[0])**2 < 8):\n",
    "        retryCount[0]+=1\n",
    "    time.sleep(waittime)\n",
    "\n",
    "def getJob():\n",
    "    data = {\n",
    "            'COMMAND': \"REQUESTJOB\",\n",
    "            'NAME' : platform.node()\n",
    "    }\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header('Content-Type', 'application/json; charset=utf-8')\n",
    "    jsondata = json.dumps(data)\n",
    "    jsondataasbytes = jsondata.encode('utf-8')\n",
    "\n",
    "    retryCount = [0]\n",
    "    while True:\n",
    "        try:\n",
    "            response = urllib.request.urlopen(req, jsondataasbytes,context=ctx)\n",
    "            retryCount = [0]\n",
    "        except:\n",
    "            #sys.stdout.write('\\rnumber is %d' %i)\n",
    "            sys.stdout.write('\\rRequesting job...')\n",
    "            sys.stdout.flush()\n",
    "            bouncingSleep(retryCount)\n",
    "            continue   \n",
    "        data = json.loads(response.read().decode('utf-8'))\n",
    "        if data[\"RESPONSE\"] == \"ASSIGNED\":            \n",
    "            return data\n",
    "        elif data[\"RESPONSE\"] == \"FINISHED\":\n",
    "            sys.stdout.write('\\rNo more job to do. listening for any further...')\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            bouncingSleep(retryCount)\n",
    "            continue  \n",
    "        elif data[\"RESPONSE\"] == \"WAIT\":\n",
    "            sys.stdout.write('\\rWait')\n",
    "            sys.stdout.flush()\n",
    "            bouncingSleep(retryCount)\n",
    "            continue         \n",
    "        else:    \n",
    "            bouncingSleep(retryCount)\n",
    "            continue  \n",
    "def submitResult(result):\n",
    "    _data = {\n",
    "            'COMMAND': \"SUBMITRESULT\",\n",
    "    }\n",
    "    data = {**_data , **result}\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header('Content-Type', 'application/json; charset=utf-8')\n",
    "    jsondata = json.dumps(data)\n",
    "    jsondataasbytes = jsondata.encode('utf-8')\n",
    "    retryCount = [0]\n",
    "    while True:\n",
    "        try:\n",
    "            response = urllib.request.urlopen(req, jsondataasbytes,context=ctx)\n",
    "            retryCount = [0]\n",
    "        except:\n",
    "            retryCount += 1\n",
    "            print(\"Submitting failed. Retry #\" + str(retry))\n",
    "            bouncingSleep(retryCount)\n",
    "            continue   \n",
    "        data = json.loads(response.read().decode('utf-8'))\n",
    "        if \"RESPONSE\" in data:\n",
    "            return data\n",
    "        else:\n",
    "            raise Exception(\"unknown response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paramsCleanup(params):\n",
    "    params[\"sae_hidden_layers\"] =  params[\"sae_hidden_layers\"].replace(\"[\",\"\")\n",
    "    params[\"sae_hidden_layers\"] =  params[\"sae_hidden_layers\"].replace(\"]\",\"\")\n",
    "    params[\"sae_hidden_layers\"] =  params[\"sae_hidden_layers\"].replace(\" \",\"\")\n",
    "    params[\"classifier_hidden_layers\"] =  params[\"classifier_hidden_layers\"].replace(\"[\",\"\")\n",
    "    params[\"classifier_hidden_layers\"] =  params[\"classifier_hidden_layers\"].replace(\"]\",\"\")\n",
    "    params[\"classifier_hidden_layers\"] =  params[\"classifier_hidden_layers\"].replace(\" \",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters retrived [2018-03-28 13:14:19]\n",
      "**************************************************\n",
      "                   Parameter                Value\n",
      "0                random_seed                    0\n",
      "1             training_ratio                  0.9\n",
      "2                     epochs                   20\n",
      "3                 batch_size                   10\n",
      "4                          N                    8\n",
      "5                    scaling                  0.2\n",
      "6          sae_hidden_layers       64,128,256,256\n",
      "7             sae_activation                 relu\n",
      "8                   sae_bias                FALSE\n",
      "9              sae_optimizer                 adam\n",
      "10                  sae_loss                  mse\n",
      "11  classifier_hidden_layers               64,128\n",
      "12     classifier_activation                 relu\n",
      "13           classifier_bias                FALSE\n",
      "14      classifier_optimizer                 adam\n",
      "15           classifier_loss  binary_crossentropy\n",
      "16                   dropout                  0.2\n",
      "17                   acc_bld                    0\n",
      "18                   acc_flr                    0\n",
      "19                    acc_bf                    0\n",
      "20               loc_failure                    0\n",
      "21              mean_pos_err                    0\n",
      "22     mean_pos_err_weighted                    0\n",
      "23                trained_by      DESKTOP-ET0IUTD\n",
      "24                time_spent                    0\n",
      "25            submitted_date  0001-01-01T00:00:00\n",
      "26                  RESPONSE             ASSIGNED\n",
      "**************************************************\n",
      "Epoch 1/20\n",
      "17943/17943 [==============================] - 7s 383us/step - loss: 0.8409\n",
      "Epoch 2/20\n",
      "17943/17943 [==============================] - 6s 350us/step - loss: 0.7476\n",
      "Epoch 3/20\n",
      "17943/17943 [==============================] - 7s 366us/step - loss: 0.7152\n",
      "Epoch 4/20\n",
      "17943/17943 [==============================] - 6s 350us/step - loss: 0.6931\n",
      "Epoch 5/20\n",
      "17943/17943 [==============================] - 6s 352us/step - loss: 0.6838\n",
      "Epoch 6/20\n",
      "17943/17943 [==============================] - 6s 349us/step - loss: 0.6721\n",
      "Epoch 7/20\n",
      "17943/17943 [==============================] - 7s 364us/step - loss: 0.6612\n",
      "Epoch 8/20\n",
      "17943/17943 [==============================] - 6s 352us/step - loss: 0.6555\n",
      "Epoch 9/20\n",
      "17943/17943 [==============================] - 6s 352us/step - loss: 0.6488\n",
      "Epoch 10/20\n",
      "17943/17943 [==============================] - 7s 363us/step - loss: 0.6391\n",
      "Epoch 11/20\n",
      "17943/17943 [==============================] - 6s 344us/step - loss: 0.6371\n",
      "Epoch 12/20\n",
      "17943/17943 [==============================] - 6s 343us/step - loss: 0.6348\n",
      "Epoch 13/20\n",
      "17943/17943 [==============================] - 6s 342us/step - loss: 0.6353\n",
      "Epoch 14/20\n",
      "17943/17943 [==============================] - 6s 342us/step - loss: 0.6272\n",
      "Epoch 15/20\n",
      "17943/17943 [==============================] - 6s 343us/step - loss: 0.6211\n",
      "Epoch 16/20\n",
      "17943/17943 [==============================] - 6s 343us/step - loss: 0.6154\n",
      "Epoch 17/20\n",
      "17943/17943 [==============================] - 6s 345us/step - loss: 0.6107\n",
      "Epoch 18/20\n",
      "17943/17943 [==============================] - 6s 348us/step - loss: 0.6021\n",
      "Epoch 19/20\n",
      "17943/17943 [==============================] - 6s 342us/step - loss: 0.6016\n",
      "Epoch 20/20\n",
      "17943/17943 [==============================] - 6s 346us/step - loss: 0.6006\n",
      "Train on 17943 samples, validate on 1994 samples\n",
      "Epoch 1/20\n",
      "17943/17943 [==============================] - 5s 268us/step - loss: 0.0668 - acc: 0.9837 - val_loss: 0.0715 - val_acc: 0.9824\n",
      "Epoch 2/20\n",
      "17943/17943 [==============================] - 4s 215us/step - loss: 0.0447 - acc: 0.9896 - val_loss: 0.0607 - val_acc: 0.9849\n",
      "Epoch 3/20\n",
      "17943/17943 [==============================] - 4s 210us/step - loss: 0.0380 - acc: 0.9903 - val_loss: 0.0568 - val_acc: 0.9848\n",
      "Epoch 4/20\n",
      "17943/17943 [==============================] - 4s 215us/step - loss: 0.0349 - acc: 0.9904 - val_loss: 0.0593 - val_acc: 0.9858\n",
      "Epoch 5/20\n",
      "17943/17943 [==============================] - 4s 210us/step - loss: 0.0320 - acc: 0.9907 - val_loss: 0.0564 - val_acc: 0.9864\n",
      "Epoch 6/20\n",
      "17943/17943 [==============================] - 4s 210us/step - loss: 0.0308 - acc: 0.9909 - val_loss: 0.0536 - val_acc: 0.9874\n",
      "Epoch 7/20\n",
      "17943/17943 [==============================] - 4s 210us/step - loss: 0.0296 - acc: 0.9910 - val_loss: 0.0513 - val_acc: 0.9881\n",
      "Epoch 8/20\n",
      "17943/17943 [==============================] - 4s 215us/step - loss: 0.0281 - acc: 0.9912 - val_loss: 0.0553 - val_acc: 0.9879\n",
      "Epoch 9/20\n",
      "17943/17943 [==============================] - 4s 223us/step - loss: 0.0276 - acc: 0.9912 - val_loss: 0.0537 - val_acc: 0.9874\n",
      "Epoch 10/20\n",
      "17943/17943 [==============================] - 4s 219us/step - loss: 0.0269 - acc: 0.9913 - val_loss: 0.0515 - val_acc: 0.9889\n",
      "Epoch 11/20\n",
      "17943/17943 [==============================] - 4s 213us/step - loss: 0.0264 - acc: 0.9914 - val_loss: 0.0536 - val_acc: 0.9886\n",
      "Epoch 12/20\n",
      "17943/17943 [==============================] - 4s 224us/step - loss: 0.0258 - acc: 0.9915 - val_loss: 0.0562 - val_acc: 0.9885\n",
      "Epoch 13/20\n",
      "17943/17943 [==============================] - 4s 240us/step - loss: 0.0257 - acc: 0.9915 - val_loss: 0.0523 - val_acc: 0.9888\n",
      "Epoch 14/20\n",
      "17943/17943 [==============================] - 4s 220us/step - loss: 0.0252 - acc: 0.9916 - val_loss: 0.0543 - val_acc: 0.9887\n",
      "Epoch 15/20\n",
      "17943/17943 [==============================] - 4s 212us/step - loss: 0.0250 - acc: 0.9916 - val_loss: 0.0551 - val_acc: 0.9890\n",
      "Epoch 16/20\n",
      "17943/17943 [==============================] - 4s 218us/step - loss: 0.0245 - acc: 0.9917 - val_loss: 0.0532 - val_acc: 0.9885\n",
      "Epoch 17/20\n",
      "17943/17943 [==============================] - 4s 225us/step - loss: 0.0244 - acc: 0.9918 - val_loss: 0.0577 - val_acc: 0.9886\n",
      "Epoch 18/20\n",
      "17943/17943 [==============================] - 4s 229us/step - loss: 0.0241 - acc: 0.9918 - val_loss: 0.0510 - val_acc: 0.9886\n",
      "Epoch 19/20\n",
      "17943/17943 [==============================] - 4s 220us/step - loss: 0.0241 - acc: 0.9919 - val_loss: 0.0543 - val_acc: 0.9885\n",
      "Epoch 20/20\n",
      "17943/17943 [==============================] - 4s 224us/step - loss: 0.0236 - acc: 0.9919 - val_loss: 0.0545 - val_acc: 0.9885\n",
      "result submitted [2018-03-28 13:18:14]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = getJob()\n",
    "paramsCleanup(params)\n",
    "\n",
    "print(\"parameters retrived \" + strftime(\"[%Y-%m-%d %H:%M:%S]\", gmtime()))\n",
    "print ('*' * 50)\n",
    "print(pd.DataFrame(list(params.items()), columns=['Parameter', 'Value']))\n",
    "print ('*' * 50)\n",
    "\n",
    "N = int(params[\"N\"])\n",
    "acc_bf = str(params[\"acc_bf\"])\n",
    "acc_bld = str(params[\"acc_bld\"])\n",
    "acc_flr = str(params[\"acc_flr\"])\n",
    "batch_size = int(params[\"batch_size\"])\n",
    "classifier_activation= str(params[\"classifier_activation\"])\n",
    "classifier_bias= str(params[\"classifier_bias\"])\n",
    "classifier_hidden_layers= str(params[\"classifier_hidden_layers\"])\n",
    "classifier_loss= str(params[\"classifier_loss\"])\n",
    "classifier_optimizer= str(params[\"classifier_optimizer\"])\n",
    "dropout= float(params[\"dropout\"])\n",
    "epochs= int(params[\"epochs\"])\n",
    "loc_failure= str(params[\"loc_failure\"])\n",
    "mean_pos_err= str(params[\"mean_pos_err\"])\n",
    "mean_pos_err_weighted= str(params[\"mean_pos_err_weighted\"])\n",
    "random_seed= int(params[\"random_seed\"])\n",
    "sae_activation= str(params[\"sae_activation\"])\n",
    "sae_bias= str(params[\"sae_bias\"])\n",
    "sae_hidden_layers= str(params[\"sae_hidden_layers\"])\n",
    "sae_loss= str(params[\"sae_loss\"])\n",
    "sae_optimizer= str(params[\"sae_optimizer\"])\n",
    "scaling= float(params[\"scaling\"])\n",
    "submitted_date= str(params[\"submitted_date\"])\n",
    "time_spent= int(params[\"time_spent\"])\n",
    "trained_by= str(params[\"trained_by\"])\n",
    "training_ratio= float(params[\"training_ratio\"])\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import scale\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "\n",
    "input_dim = 520\n",
    "output_dim = 13                 # number of labels\n",
    "verbose = 1                     # 0 for turning off logging\n",
    "INPUT_DIM = 520                 #  number of APs\n",
    "\n",
    "path_train = 'UJIIndoorLoc/trainingData2.csv'           # '-110' for the lack of AP.\n",
    "path_validation = 'UJIIndoorLoc/validationData2.csv'    # ditto\n",
    "\n",
    "\n",
    "sae_hidden_layers =  [int(i) for i in (sae_hidden_layers).split(',')]\n",
    "classifier_hidden_layers =  [int(i) for i in (classifier_hidden_layers).split(',')]\n",
    "\n",
    "\n",
    "import random as rn\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(int(random_seed))\n",
    "rn.seed(12345)\n",
    "\n",
    "import tensorflow as tf\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(random_seed)  # initialize random seed generator of tensorflow\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)   \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "train_df = pd.read_csv(path_train, header=0) # pass header=0 to be able to replace existing names\n",
    "test_df = pd.read_csv(path_validation, header=0)\n",
    "\n",
    "train_AP_features = scale(np.asarray(train_df.iloc[:,0:520]).astype(float), axis=1)\n",
    "\n",
    "# add a new column\n",
    "train_df['REFPOINT'] = train_df.apply(lambda row: str(int(row['SPACEID'])) + str(int(row['RELATIVEPOSITION'])), axis=1)\n",
    "\n",
    "blds = np.unique(train_df[['BUILDINGID']])\n",
    "flrs = np.unique(train_df[['FLOOR']])\n",
    "\n",
    "x_avg = {}\n",
    "y_avg = {}\n",
    "for bld in blds:\n",
    "    for flr in flrs:\n",
    "        # map reference points to sequential IDs per building-floor before building labels\n",
    "        cond = (train_df['BUILDINGID']==bld) & (train_df['FLOOR']==flr)\n",
    "\n",
    "        _, idx = np.unique(train_df.loc[cond, 'REFPOINT'], return_inverse=True) # refer to numpy.unique manual\n",
    "        train_df.loc[cond, 'REFPOINT'] = idx\n",
    "\n",
    "        # calculate the average coordinates of each building/floor\n",
    "        x_avg[str(bld) + '-' + str(flr)] = np.mean(train_df.loc[cond, 'LONGITUDE'])\n",
    "        y_avg[str(bld) + '-' + str(flr)] = np.mean(train_df.loc[cond, 'LATITUDE'])\n",
    "\n",
    "len_train = len(train_df) \n",
    "\n",
    "blds_all = np.asarray(pd.get_dummies(pd.concat([train_df['BUILDINGID'], test_df['BUILDINGID']])))\n",
    "flrs_all = np.asarray(pd.get_dummies(pd.concat([train_df['FLOOR'], test_df['FLOOR']]))) # ditto\n",
    "\n",
    "blds = blds_all[:len_train]\n",
    "flrs = flrs_all[:len_train]\n",
    "\n",
    "rfps = np.asarray(pd.get_dummies(train_df['REFPOINT']))\n",
    "train_labels = np.concatenate((blds, flrs, rfps), axis=1)\n",
    "OUTPUT_DIM = train_labels.shape[1]\n",
    "\n",
    "# we will use the validation set at a testing set.\n",
    "\n",
    "train_val_split = np.full((len(train_AP_features)), True)\n",
    "train_val_split[int(len(train_AP_features)*training_ratio):len(train_AP_features)*99] = False\n",
    "\n",
    "x_train = train_AP_features[train_val_split]\n",
    "y_train = train_labels[train_val_split]\n",
    "x_val = train_AP_features[~train_val_split]\n",
    "y_val = train_labels[~train_val_split]\n",
    "\n",
    "# create a model based on stacked autoencoder (SAE)\n",
    "model = Sequential()\n",
    "model.add(Dense(sae_hidden_layers[0], input_dim=INPUT_DIM, activation=sae_activation, use_bias=sae_bias))\n",
    "for units in sae_hidden_layers[1:]:\n",
    "    model.add(Dense(units, activation=sae_activation, use_bias=sae_bias))  \n",
    "model.add(Dense(INPUT_DIM, activation=sae_activation, use_bias=sae_bias))\n",
    "model.compile(optimizer=sae_optimizer, loss=sae_loss)\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=verbose,shuffle=False)\n",
    "\n",
    "# remove the decoder part\n",
    "num_to_remove = (len(sae_hidden_layers) + 1) // 2\n",
    "for i in range(num_to_remove):\n",
    "    model.pop()\n",
    "\n",
    "### build and train a complete model with the trained SAE encoder and a new classifier\n",
    "model.add(Dropout(dropout))\n",
    "for units in classifier_hidden_layers:\n",
    "    model.add(Dense(units, activation=classifier_activation, use_bias=classifier_bias))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(OUTPUT_DIM, activation='sigmoid', use_bias=classifier_bias)) # 'sigmoid' for multi-label classification\n",
    "model.compile(optimizer=classifier_optimizer, loss=classifier_loss, metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs, verbose=verbose,shuffle=False)\n",
    "\n",
    "# turn the given validation set into a testing set\n",
    "test_AP_features = scale(np.asarray(test_df.iloc[:,0:520]).astype(float), axis=1) # convert integer to float and scale jointly (axis=1)\n",
    "x_test_utm = np.asarray(test_df['LONGITUDE'])\n",
    "y_test_utm = np.asarray(test_df['LATITUDE'])\n",
    "blds = blds_all[len_train:]\n",
    "flrs = flrs_all[len_train:]\n",
    "\n",
    "### evaluate the model\n",
    "# calculate the accuracy of building and floor estimation\n",
    "preds = model.predict(test_AP_features, batch_size=batch_size)\n",
    "n_preds = preds.shape[0]\n",
    "\n",
    "# blds_results = (np.equal(np.argmax(test_labels[:, :3], axis=1), np.argmax(preds[:, :3], axis=1))).astype(int)\n",
    "blds_results = (np.equal(np.argmax(blds, axis=1), np.argmax(preds[:, :3], axis=1))).astype(int)\n",
    "acc_bld = blds_results.mean()\n",
    "\n",
    "flrs_results = (np.equal(np.argmax(flrs, axis=1), np.argmax(preds[:, 3:8], axis=1))).astype(int)\n",
    "acc_flr = flrs_results.mean()\n",
    "acc_bf = (blds_results*flrs_results).mean()\n",
    "\n",
    "# calculate positioning error when building and floor are correctly estimated\n",
    "mask = np.logical_and(blds_results, flrs_results) # mask index array for correct location of building and floor\n",
    "\n",
    "x_test_utm = x_test_utm[mask]\n",
    "y_test_utm = y_test_utm[mask]\n",
    "blds = blds[mask]\n",
    "flrs = flrs[mask]\n",
    "rfps = (preds[mask])[:, 8:118]\n",
    "\n",
    "# number of correct building and floor location\n",
    "n_success = len(blds)   \n",
    "\n",
    "n_loc_failure = 0\n",
    "sum_pos_err = 0.0\n",
    "sum_pos_err_weighted = 0.0\n",
    "idxs = np.argpartition(rfps, -N)[:, -N:]  # (unsorted) indexes of up to N nearest neighbors\n",
    "threshold = scaling*np.amax(rfps, axis=1)\n",
    "\n",
    "for i in range(n_success):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ws = []\n",
    "    for j in idxs[i]:\n",
    "        rfp = np.zeros(110)\n",
    "        rfp[j] = 1\n",
    "        rows = np.where((train_labels == np.concatenate((blds[i], flrs[i], rfp))).all(axis=1)) # tuple of row indexes\n",
    "        if rows[0].size > 0:\n",
    "            if rfps[i][j] >= threshold[i]:\n",
    "                xs.append(train_df.loc[train_df.index[rows[0][0]], 'LONGITUDE'])\n",
    "                ys.append(train_df.loc[train_df.index[rows[0][0]], 'LATITUDE'])\n",
    "                ws.append(rfps[i][j])\n",
    "    if len(xs) > 0:\n",
    "        sum_pos_err += math.sqrt((np.mean(xs)-x_test_utm[i])**2 + (np.mean(ys)-y_test_utm[i])**2)\n",
    "        sum_pos_err_weighted += math.sqrt((np.average(xs, weights=ws)-x_test_utm[i])**2 + (np.average(ys, weights=ws)-y_test_utm[i])**2)\n",
    "    else:\n",
    "        n_loc_failure += 1\n",
    "        key = str(np.argmax(blds[i])) + '-' + str(np.argmax(flrs[i]))\n",
    "        pos_err = math.sqrt((x_avg[key]-x_test_utm[i])**2 + (y_avg[key]-y_test_utm[i])**2)\n",
    "        sum_pos_err += pos_err\n",
    "        sum_pos_err_weighted += pos_err\n",
    "\n",
    "\n",
    "\n",
    "# mean_pos_err = sum_pos_err / (n_success - n_loc_failure)\n",
    "mean_pos_err = sum_pos_err / n_success\n",
    "# mean_pos_err_weighted = sum_pos_err_weighted / (n_success - n_loc_failure)\n",
    "mean_pos_err_weighted = sum_pos_err_weighted / n_success\n",
    "loc_failure = n_loc_failure / n_success # rate of location estimation failure given that building and floor are correctly located\n",
    "\n",
    "\n",
    "end = datetime.now() \n",
    "timeTaken = end - start\n",
    "output = {   \"N\": N,\n",
    "            \"acc_bf\": acc_bf,\n",
    "            \"acc_bld\": acc_bld,\n",
    "            \"acc_flr\": acc_flr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"classifier_activation\": classifier_activation,\n",
    "            \"classifier_bias\": classifier_bias,\n",
    "            \"classifier_hidden_layers\": \",\".join([str(item) for item in classifier_hidden_layers]),\n",
    "            \"classifier_loss\": classifier_loss,\n",
    "            \"classifier_optimizer\": classifier_optimizer,\n",
    "            \"dropout\": dropout,\n",
    "            \"epochs\": epochs,\n",
    "            \"loc_failure\": loc_failure,\n",
    "            \"mean_pos_err\": mean_pos_err,\n",
    "            \"mean_pos_err_weighted\": mean_pos_err_weighted,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"sae_activation\": sae_activation,\n",
    "            \"sae_bias\": sae_bias,\n",
    "            \"sae_hidden_layers\": \",\".join([str(item) for item in sae_hidden_layers]),\n",
    "            \"sae_loss\": sae_loss,\n",
    "            \"sae_optimizer\": sae_optimizer,\n",
    "            \"scaling\": scaling,\n",
    "            \"submitted_date\": datetime.now().isoformat(),\n",
    "            \"time_spent\":  int(timeTaken.total_seconds()),\n",
    "            \"trained_by\": platform.node(),\n",
    "            \"training_ratio\": training_ratio\n",
    "}\n",
    "submit = submitResult(output)\n",
    "if submit[\"RESPONSE\"] == \"SUCCESS\":            \n",
    "    print(\"result submitted \" + strftime(\"[%Y-%m-%d %H:%M:%S]\", gmtime()))\n",
    "elif submit[\"RESPONSE\"] == \"FAIL\":\n",
    "    print(\"result is rejected \" + strftime(\"[%Y-%m-%d %H:%M:%S]\", gmtime()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
