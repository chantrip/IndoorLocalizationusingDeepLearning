{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import scale\n",
    "from timeit import default_timer as timer\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### set paramter values\n",
    "#------------------------------------------------------------------------\n",
    "# general\n",
    "#------------------------------------------------------------------------\n",
    "training_ratio = 0.9            # ratio of training data to overall data\n",
    "input_dim = 520\n",
    "output_dim = 13                 # number of labels\n",
    "verbose = 1                     # 0 for turning off logging\n",
    "seed = 7                        # random number seed for reproducibility\n",
    "### global constant variables\n",
    "#------------------------------------------------------------------------\n",
    "# general\n",
    "#------------------------------------------------------------------------\n",
    "INPUT_DIM = 520                 #  number of APs\n",
    "VERBOSE = 1                     # 0 for turning off logging\n",
    "#------------------------------------------------------------------------\n",
    "# stacked auto encoder (sae)\n",
    "#------------------------------------------------------------------------\n",
    "# SAE_ACTIVATION = 'tanh'\n",
    "SAE_ACTIVATION = 'relu'\n",
    "SAE_BIAS = False\n",
    "SAE_OPTIMIZER = 'adam'\n",
    "SAE_LOSS = 'mse'\n",
    "#------------------------------------------------------------------------\n",
    "# classifier\n",
    "#------------------------------------------------------------------------\n",
    "CLASSIFIER_ACTIVATION = 'relu'\n",
    "CLASSIFIER_BIAS = False\n",
    "CLASSIFIER_OPTIMIZER = 'adam'\n",
    "CLASSIFIER_LOSS = 'binary_crossentropy'\n",
    "#------------------------------------------------------------------------\n",
    "# input files\n",
    "#------------------------------------------------------------------------\n",
    "path_train = '../data/UJIIndoorLoc/trainingData2.csv'           # '-110' for the lack of AP.\n",
    "path_validation = '../data/UJIIndoorLoc/validationData2.csv'    # ditto\n",
    "#------------------------------------------------------------------------\n",
    "# output files\n",
    "#------------------------------------------------------------------------\n",
    "path_base = '../my_results/'\n",
    "path_out =  path_base + ''\n",
    "#path_sae_model = path_base + '_sae_model.hdf5'\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 20\n",
    "#sae_hidden_layers = [256,128,64,128,256]\n",
    "sae_hidden_layers = [128,512,512,128]\n",
    "#classifier_hidden_layers = [128,128]\n",
    "classifier_hidden_layers =  [128,128,512]\n",
    "dropout = 0.2\n",
    "N = 7\n",
    "scaling= 0.0\n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### initialize random seed generator of numpy\n",
    "import random as rn\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "rn.seed(12345)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,device_count = {'GPU': 0})\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(random_seed)  # initialize random seed generator of tensorflow\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path_train, header=0) # pass header=0 to be able to replace existing names\n",
    "test_df = pd.read_csv(path_validation, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_AP_features = scale(np.asarray(train_df.iloc[:,0:520]).astype(float), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17125017, -0.17125017, -0.17125017, ..., -0.17125017,\n",
       "        -0.17125017, -0.17125017],\n",
       "       [-0.16059846, -0.16059846, -0.16059846, ..., -0.16059846,\n",
       "        -0.16059846, -0.16059846],\n",
       "       [-0.15523113, -0.15523113, -0.15523113, ..., -0.15523113,\n",
       "        -0.15523113, -0.15523113],\n",
       "       ..., \n",
       "       [-0.1077911 , -0.1077911 , -0.1077911 , ..., -0.1077911 ,\n",
       "        -0.1077911 , -0.1077911 ],\n",
       "       [-0.17141826, -0.17141826, -0.17141826, ..., -0.17141826,\n",
       "        -0.17141826, -0.17141826],\n",
       "       [-0.17331788, -0.17331788, -0.17331788, ..., -0.17331788,\n",
       "        -0.17331788, -0.17331788]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_AP_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # add a new column\n",
    "train_df['REFPOINT'] = train_df.apply(lambda row: str(int(row['SPACEID'])) + str(int(row['RELATIVEPOSITION'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blds = np.unique(train_df[['BUILDINGID']])\n",
    "flrs = np.unique(train_df[['FLOOR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_avg = {}\n",
    "y_avg = {}\n",
    "for bld in blds:\n",
    "    for flr in flrs:\n",
    "        # map reference points to sequential IDs per building-floor before building labels\n",
    "        cond = (train_df['BUILDINGID']==bld) & (train_df['FLOOR']==flr)\n",
    "        \n",
    "        _, idx = np.unique(train_df.loc[cond, 'REFPOINT'], return_inverse=True) # refer to numpy.unique manual\n",
    "        train_df.loc[cond, 'REFPOINT'] = idx\n",
    "            \n",
    "        # calculate the average coordinates of each building/floor\n",
    "        x_avg[str(bld) + '-' + str(flr)] = np.mean(train_df.loc[cond, 'LONGITUDE'])\n",
    "        y_avg[str(bld) + '-' + str(flr)] = np.mean(train_df.loc[cond, 'LATITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19937"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_train = len(train_df) \n",
    "len_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for consistency in one-hot encoding\n",
    "blds_all = np.asarray(pd.get_dummies(pd.concat([train_df['BUILDINGID'], test_df['BUILDINGID']])))\n",
    "flrs_all = np.asarray(pd.get_dummies(pd.concat([train_df['FLOOR'], test_df['FLOOR']]))) # ditto\n",
    "\n",
    "blds = blds_all[:len_train]\n",
    "flrs = flrs_all[:len_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfps = np.asarray(pd.get_dummies(train_df['REFPOINT']))\n",
    "train_labels = np.concatenate((blds, flrs, rfps), axis=1)\n",
    "OUTPUT_DIM = train_labels.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the training set into training and validation sets; \n",
    "\n",
    "# we will use the validation set at a testing set.\n",
    "train_val_split = np.full((len(train_AP_features)), True)\n",
    "train_val_split[int(len(train_AP_features)*training_ratio):len(train_AP_features)*99] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_AP_features[train_val_split]\n",
    "y_train = train_labels[train_val_split]\n",
    "x_val = train_AP_features[~train_val_split]\n",
    "y_val = train_labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17943/17943 [==============================] - 12s 685us/step - loss: 0.8484\n",
      "Epoch 2/20\n",
      "17943/17943 [==============================] - 12s 678us/step - loss: 0.7144\n",
      "Epoch 3/20\n",
      "17943/17943 [==============================] - 12s 662us/step - loss: 0.6585\n",
      "Epoch 4/20\n",
      "17943/17943 [==============================] - 12s 649us/step - loss: 0.6393\n",
      "Epoch 5/20\n",
      "17943/17943 [==============================] - 12s 661us/step - loss: 0.6151\n",
      "Epoch 6/20\n",
      "17943/17943 [==============================] - 12s 664us/step - loss: 0.5883\n",
      "Epoch 7/20\n",
      "17943/17943 [==============================] - 13s 710us/step - loss: 0.5775\n",
      "Epoch 8/20\n",
      "17943/17943 [==============================] - 12s 657us/step - loss: 0.5622\n",
      "Epoch 9/20\n",
      "17943/17943 [==============================] - 12s 660us/step - loss: 0.5572\n",
      "Epoch 10/20\n",
      "17943/17943 [==============================] - 12s 659us/step - loss: 0.5502\n",
      "Epoch 11/20\n",
      "17943/17943 [==============================] - 12s 669us/step - loss: 0.5338\n",
      "Epoch 12/20\n",
      "17943/17943 [==============================] - 12s 670us/step - loss: 0.5252\n",
      "Epoch 13/20\n",
      "17943/17943 [==============================] - 13s 708us/step - loss: 0.5213\n",
      "Epoch 14/20\n",
      "17943/17943 [==============================] - 14s 766us/step - loss: 0.5149\n",
      "Epoch 15/20\n",
      "17943/17943 [==============================] - 13s 700us/step - loss: 0.5064\n",
      "Epoch 16/20\n",
      "17943/17943 [==============================] - 12s 693us/step - loss: 0.4940\n",
      "Epoch 17/20\n",
      "17943/17943 [==============================] - 12s 692us/step - loss: 0.4839\n",
      "Epoch 18/20\n",
      "17943/17943 [==============================] - 12s 682us/step - loss: 0.4803\n",
      "Epoch 19/20\n",
      "17943/17943 [==============================] - 13s 717us/step - loss: 0.4801\n",
      "Epoch 20/20\n",
      "17943/17943 [==============================] - 12s 682us/step - loss: 0.4698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x244a32165f8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model based on stacked autoencoder (SAE)\n",
    "model = Sequential()\n",
    "model.add(Dense(sae_hidden_layers[0], input_dim=INPUT_DIM, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))\n",
    "for units in sae_hidden_layers[1:]:\n",
    "    model.add(Dense(units, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))  \n",
    "model.add(Dense(INPUT_DIM, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))\n",
    "model.compile(optimizer=SAE_OPTIMIZER, loss=SAE_LOSS)\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=VERBOSE,shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the decoder part\n",
    "num_to_remove = (len(sae_hidden_layers) + 1) // 2\n",
    "for i in range(num_to_remove):\n",
    "    model.pop()\n",
    "    \n",
    "### build and train a complete model with the trained SAE encoder and a new classifier\n",
    "model.add(Dropout(dropout))\n",
    "for units in classifier_hidden_layers:\n",
    "    model.add(Dense(units, activation=CLASSIFIER_ACTIVATION, use_bias=CLASSIFIER_BIAS))\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(OUTPUT_DIM, activation='sigmoid', use_bias=CLASSIFIER_BIAS)) # 'sigmoid' for multi-label classification\n",
    "model.compile(optimizer=CLASSIFIER_OPTIMIZER, loss=CLASSIFIER_LOSS, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17943 samples, validate on 1994 samples\n",
      "Epoch 1/20\n",
      "17943/17943 [==============================] - 15s 851us/step - loss: 0.0658 - acc: 0.9859 - val_loss: 0.0939 - val_acc: 0.9783\n",
      "Epoch 2/20\n",
      "17943/17943 [==============================] - 15s 830us/step - loss: 0.0498 - acc: 0.9888 - val_loss: 0.0771 - val_acc: 0.9827\n",
      "Epoch 3/20\n",
      "17943/17943 [==============================] - 15s 858us/step - loss: 0.0433 - acc: 0.9895 - val_loss: 0.0777 - val_acc: 0.9821\n",
      "Epoch 4/20\n",
      "17943/17943 [==============================] - 15s 831us/step - loss: 0.0404 - acc: 0.9897 - val_loss: 0.0635 - val_acc: 0.9839\n",
      "Epoch 5/20\n",
      "17943/17943 [==============================] - 16s 875us/step - loss: 0.0377 - acc: 0.9900 - val_loss: 0.0618 - val_acc: 0.9844\n",
      "Epoch 6/20\n",
      "17943/17943 [==============================] - 16s 912us/step - loss: 0.0360 - acc: 0.9902 - val_loss: 0.0671 - val_acc: 0.9851\n",
      "Epoch 7/20\n",
      "17943/17943 [==============================] - 15s 863us/step - loss: 0.0352 - acc: 0.9903 - val_loss: 0.0635 - val_acc: 0.9856\n",
      "Epoch 8/20\n",
      "17943/17943 [==============================] - 15s 818us/step - loss: 0.0344 - acc: 0.9904 - val_loss: 0.0582 - val_acc: 0.9867\n",
      "Epoch 9/20\n",
      "17943/17943 [==============================] - 15s 851us/step - loss: 0.0331 - acc: 0.9906 - val_loss: 0.0631 - val_acc: 0.9864\n",
      "Epoch 10/20\n",
      "17943/17943 [==============================] - 15s 858us/step - loss: 0.0329 - acc: 0.9907 - val_loss: 0.0573 - val_acc: 0.9866\n",
      "Epoch 11/20\n",
      "17943/17943 [==============================] - 15s 862us/step - loss: 0.0325 - acc: 0.9908 - val_loss: 0.0565 - val_acc: 0.9877\n",
      "Epoch 12/20\n",
      "17943/17943 [==============================] - 15s 862us/step - loss: 0.0325 - acc: 0.9908 - val_loss: 0.0611 - val_acc: 0.9866\n",
      "Epoch 13/20\n",
      "17943/17943 [==============================] - 16s 867us/step - loss: 0.0322 - acc: 0.9909 - val_loss: 0.0598 - val_acc: 0.9873\n",
      "Epoch 14/20\n",
      "17943/17943 [==============================] - 15s 850us/step - loss: 0.0319 - acc: 0.9909 - val_loss: 0.0577 - val_acc: 0.9878\n",
      "Epoch 15/20\n",
      "17943/17943 [==============================] - 15s 847us/step - loss: 0.0318 - acc: 0.9908 - val_loss: 0.0641 - val_acc: 0.9879\n",
      "Epoch 16/20\n",
      "17943/17943 [==============================] - 15s 844us/step - loss: 0.0322 - acc: 0.9908 - val_loss: 0.0611 - val_acc: 0.9881\n",
      "Epoch 17/20\n",
      "17943/17943 [==============================] - 15s 856us/step - loss: 0.0319 - acc: 0.9909 - val_loss: 0.0567 - val_acc: 0.9882\n",
      "Epoch 18/20\n",
      "17943/17943 [==============================] - 15s 833us/step - loss: 0.0319 - acc: 0.9909 - val_loss: 0.0632 - val_acc: 0.9883\n",
      "Epoch 19/20\n",
      "17943/17943 [==============================] - 15s 851us/step - loss: 0.0322 - acc: 0.9909 - val_loss: 0.0588 - val_acc: 0.9880\n",
      "Epoch 20/20\n",
      "17943/17943 [==============================] - 15s 825us/step - loss: 0.0321 - acc: 0.9909 - val_loss: 0.0630 - val_acc: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x244b9a56a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs, verbose=VERBOSE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "602112"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn the given validation set into a testing set\n",
    "test_AP_features = scale(np.asarray(test_df.iloc[:,0:520]).astype(float), axis=1) # convert integer to float and scale jointly (axis=1)\n",
    "x_test_utm = np.asarray(test_df['LONGITUDE'])\n",
    "y_test_utm = np.asarray(test_df['LATITUDE'])\n",
    "blds = blds_all[len_train:]\n",
    "flrs = flrs_all[len_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.81000662e-01,   5.06166697e-01,   4.81800199e-01],\n",
       "       [  7.14527881e-11,   8.98200792e-10,   1.00000000e+00],\n",
       "       [  1.20500936e-11,   3.72091385e-10,   1.00000000e+00],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   5.94304891e-16,   3.61344899e-14],\n",
       "       [  1.00000000e+00,   7.75575554e-15,   3.11292096e-14],\n",
       "       [  1.00000000e+00,   1.17665353e-14,   1.64118041e-13]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### evaluate the model\n",
    "# calculate the accuracy of building and floor estimation\n",
    "preds = model.predict(test_AP_features, batch_size=batch_size)\n",
    "n_preds = preds.shape[0]\n",
    "\n",
    "#np.savetxt('building_score_cpu_4.csv',preds[:, :3], delimiter=',')\n",
    "preds[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# blds_results = (np.equal(np.argmax(test_labels[:, :3], axis=1), np.argmax(preds[:, :3], axis=1))).astype(int)\n",
    "blds_results = (np.equal(np.argmax(blds, axis=1), np.argmax(preds[:, :3], axis=1))).astype(int)\n",
    "acc_bld = blds_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(blds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flrs_results = (np.equal(np.argmax(flrs, axis=1), np.argmax(preds[:, 3:8], axis=1))).astype(int)\n",
    "acc_flr = flrs_results.mean()\n",
    "acc_bf = (blds_results*flrs_results).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91359135913591361"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate positioning error when building and floor are correctly estimated\n",
    "mask = np.logical_and(blds_results, flrs_results) # mask index array for correct location of building and floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_utm = x_test_utm[mask]\n",
    "y_test_utm = y_test_utm[mask]\n",
    "blds = blds[mask]\n",
    "flrs = flrs[mask]\n",
    "rfps = (preds[mask])[:, 8:118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.81000662e-01,   5.06166697e-01,   4.81800199e-01, ...,\n",
       "          2.72699803e-01,   2.78551728e-01,   2.31499851e-01],\n",
       "       [  7.14527881e-11,   8.98200792e-10,   1.00000000e+00, ...,\n",
       "          4.46654568e-17,   6.61485108e-21,   2.41057187e-22],\n",
       "       [  1.20500936e-11,   3.72091385e-10,   1.00000000e+00, ...,\n",
       "          8.72345765e-15,   6.28601483e-19,   3.33824301e-22],\n",
       "       ..., \n",
       "       [  1.00000000e+00,   5.94304891e-16,   3.61344899e-14, ...,\n",
       "          1.03163665e-31,   2.06366598e-29,   7.66493396e-29],\n",
       "       [  1.00000000e+00,   7.75575554e-15,   3.11292096e-14, ...,\n",
       "          6.78303311e-31,   2.72718421e-29,   1.62613042e-28],\n",
       "       [  1.00000000e+00,   1.17665353e-14,   1.64118041e-13, ...,\n",
       "          9.32422526e-31,   8.62778458e-29,   3.78770933e-28]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of correct building and floor location\n",
    "n_success = len(blds)   \n",
    "n_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_loc_failure = 0\n",
    "sum_pos_err = 0.0\n",
    "sum_pos_err_weighted = 0.0\n",
    "idxs = np.argpartition(rfps, -N)[:, -N:]  # (unsorted) indexes of up to N nearest neighbors\n",
    "threshold = scaling*np.amax(rfps, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61, 47, 54, ..., 60, 62, 57],\n",
       "       [ 4,  9, 23, ...,  8, 12, 10],\n",
       "       [26,  8,  9, ..., 12, 23, 10],\n",
       "       ..., \n",
       "       [47, 41, 42, ..., 44, 45, 43],\n",
       "       [47, 41, 44, ..., 45, 43, 42],\n",
       "       [47, 41, 44, ..., 45, 43, 42]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 79, 109,   2, ...,  60,  62,  57],\n",
       "       [ 65, 109, 108, ...,   8,  12,  10],\n",
       "       [ 75, 109, 108, ...,  12,  23,  10],\n",
       "       ..., \n",
       "       [ 64, 109, 108, ...,  44,  45,  43],\n",
       "       [ 55, 109, 108, ...,  45,  43,  42],\n",
       "       [ 60, 109, 108, ...,  45,  43,  42]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argpartition(rfps, -N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61, 47, 54, ..., 60, 62, 57],\n",
       "       [ 4,  9, 23, ...,  8, 12, 10],\n",
       "       [26,  8,  9, ..., 12, 23, 10],\n",
       "       ..., \n",
       "       [47, 41, 42, ..., 44, 45, 43],\n",
       "       [47, 41, 44, ..., 45, 43, 42],\n",
       "       [47, 41, 44, ..., 45, 43, 42]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argpartition(rfps, -N)[:, -N:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015, 110)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(n_success):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ws = []\n",
    "    for j in idxs[i]:\n",
    "        rfp = np.zeros(110)\n",
    "        rfp[j] = 1\n",
    "        rows = np.where((train_labels == np.concatenate((blds[i], flrs[i], rfp))).all(axis=1)) # tuple of row indexes\n",
    "        if rows[0].size > 0:\n",
    "            if rfps[i][j] >= threshold[i]:\n",
    "                xs.append(train_df.loc[train_df.index[rows[0][0]], 'LONGITUDE'])\n",
    "                ys.append(train_df.loc[train_df.index[rows[0][0]], 'LATITUDE'])\n",
    "                ws.append(rfps[i][j])\n",
    "    if len(xs) > 0:\n",
    "        sum_pos_err += math.sqrt((np.mean(xs)-x_test_utm[i])**2 + (np.mean(ys)-y_test_utm[i])**2)\n",
    "        sum_pos_err_weighted += math.sqrt((np.average(xs, weights=ws)-x_test_utm[i])**2 + (np.average(ys, weights=ws)-y_test_utm[i])**2)\n",
    "    else:\n",
    "        n_loc_failure += 1\n",
    "        key = str(np.argmax(blds[i])) + '-' + str(np.argmax(flrs[i]))\n",
    "        pos_err = math.sqrt((x_avg[key]-x_test_utm[i])**2 + (y_avg[key]-y_test_utm[i])**2)\n",
    "        sum_pos_err += pos_err\n",
    "        sum_pos_err_weighted += pos_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # mean_pos_err = sum_pos_err / (n_success - n_loc_failure)\n",
    "mean_pos_err = sum_pos_err / n_success\n",
    "# mean_pos_err_weighted = sum_pos_err_weighted / (n_success - n_loc_failure)\n",
    "mean_pos_err_weighted = sum_pos_err_weighted / n_success\n",
    "loc_failure = n_loc_failure / n_success # rate of location estimation failure given that building and floor are correctly located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### print out final results\n",
    "now = datetime.datetime.now\n",
    "path_out = \"../my_results/\"\n",
    "#path_out += \"_\" + now.strftime(\"%Y%m%d-%H%M%S\") + \".txt\"\n",
    "path_out += \"[SAE\" + str(sae_hidden_layers) + \"] \"\n",
    "path_out += \"[Class\" +str(classifier_hidden_layers) +  \"] \"\n",
    "path_out += \"[DropOut\" + str(dropout)+\"] \"\n",
    "path_out += \"[PE\" + str(round(mean_pos_err,2))+\"] \"\n",
    "path_out += \"[PEW\" + str(round(mean_pos_err_weighted,2))+\"] \"\n",
    "path_out += \".txt\"\n",
    "f = open(path_out, 'w')\n",
    "f.write(\"#+STARTUP: showall\\n\")  # unfold everything when opening\n",
    "f.write(\"* System parameters\\n\")\n",
    "f.write(\"  - Numpy random number seed: %d\\n\" % random_seed)\n",
    "f.write(\"  - Ratio of training data to overall data: %.2f\\n\" % training_ratio)\n",
    "f.write(\"  - Number of epochs: %d\\n\" % epochs)\n",
    "f.write(\"  - Batch size: %d\\n\" % batch_size)\n",
    "f.write(\"  - Number of neighbours: %d\\n\" % N)\n",
    "f.write(\"  - Scaling factor for threshold: %.2f\\n\" % scaling)\n",
    "f.write(\"  - SAE hidden layers: %d\" % sae_hidden_layers[0])\n",
    "for units in sae_hidden_layers[1:]:\n",
    "    f.write(\"-%d\" % units)\n",
    "f.write(\"\\n\")\n",
    "f.write(\"  - SAE activation: %s\\n\" % SAE_ACTIVATION)\n",
    "f.write(\"  - SAE bias: %s\\n\" % SAE_BIAS)\n",
    "f.write(\"  - SAE optimizer: %s\\n\" % SAE_OPTIMIZER)\n",
    "f.write(\"  - SAE loss: %s\\n\" % SAE_LOSS)\n",
    "f.write(\"  - Classifier hidden layers: \")\n",
    "if classifier_hidden_layers == '':\n",
    "    f.write(\"N/A\\n\")\n",
    "else:\n",
    "    f.write(\"%d\" % classifier_hidden_layers[0])\n",
    "    for units in classifier_hidden_layers[1:]:\n",
    "        f.write(\"-%d\" % units)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"  - Classifier hidden layer activation: %s\\n\" % CLASSIFIER_ACTIVATION)\n",
    "f.write(\"  - Classifier bias: %s\\n\" % CLASSIFIER_BIAS)\n",
    "f.write(\"  - Classifier optimizer: %s\\n\" % CLASSIFIER_OPTIMIZER)\n",
    "f.write(\"  - Classifier loss: %s\\n\" % CLASSIFIER_LOSS)\n",
    "f.write(\"  - Classifier dropout rate: %.2f\\n\" % dropout)\n",
    "# f.write(\"  - Classifier class weight for buildings: %.2f\\n\" % building_weight)\n",
    "# f.write(\"  - Classifier class weight for floors: %.2f\\n\" % floor_weight)\n",
    "f.write(\"* Performance\\n\")\n",
    "f.write(\"  - Accuracy (building): %e\\n\" % acc_bld)\n",
    "f.write(\"  - Accuracy (floor): %e\\n\" % acc_flr)\n",
    "f.write(\"  - Accuracy (building-floor): %e\\n\" % acc_bf)\n",
    "f.write(\"  - Location estimation failure rate (given the correct building/floor): %e\\n\" % loc_failure)\n",
    "f.write(\"  - Positioning error (meter): %e\\n\" % mean_pos_err)\n",
    "f.write(\"  - Positioning error (weighted; meter): %e\\n\" % mean_pos_err_weighted)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.158071617355057"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pos_err_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#9.030509479537267"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
